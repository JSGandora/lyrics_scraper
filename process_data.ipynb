{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP IMPORTS\n",
    "\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "GENERATE_DATA = False\n",
    "GENRES = [\"blues\", \"gospel\", \"rap\", \"country\", \"rock\"]\n",
    "DATA_DIR = \"/n/fs/guoweis-18iw/get_data/lyrics\"\n",
    "ALL_DATA_FN = \"all.data\"\n",
    "\n",
    "if GENERATE_DATA:\n",
    "    # Read data\n",
    "    df = pd.DataFrame(np.nan, index=[], columns=['artist', 'title', 'album', 'year', 'lyrics', 'genre'])\n",
    "\n",
    "    ct = 0\n",
    "    for genre in GENRES:\n",
    "        genre_dir = join(DATA_DIR, genre)\n",
    "        fns = listdir(genre_dir)\n",
    "        for i, fn in enumerate(fns):\n",
    "            if i % 10 == 0:\n",
    "                print(\"Done with \" + str(i) + \" of \" + str(len(fns)) + \" files.\")\n",
    "            fp = join(genre_dir, fn)\n",
    "            data_str = open(fp).read()\n",
    "            data = json.loads(data_str)\n",
    "            songs_data = data[\"songs\"]\n",
    "            for j, song in enumerate(songs_data):\n",
    "                df.loc[ct, \"genre\"] = genre\n",
    "                for key in song.keys():\n",
    "                    if key == \"raw\" or key == \"image\":\n",
    "                        continue\n",
    "                    df.loc[ct, key] = song[key]\n",
    "                ct += 1\n",
    "        df.to_pickle(genre + \".data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "ACCEPTED_CHARS = set('abcdefghijklmnopqrstuvwxyz \\'\\n')\n",
    "def lyrics_strip(lyrics):\n",
    "    verse = lyrics.strip().lower()\n",
    "    verse = ''.join(filter(ACCEPTED_CHARS.__contains__, verse))\n",
    "    return verse\n",
    "\n",
    "def lyrics_to_linelist(lyrics):\n",
    "    lyrics = lyrics.split(\"\\n\")\n",
    "    lyrics = [line.split() for line in lyrics]\n",
    "    return lyrics\n",
    "\n",
    "def linelist_to_wordlist(lines):\n",
    "    return [word for line in lines for word in line]\n",
    "\n",
    "def lyrics_to_wordlist(lyrics):\n",
    "    return lyrics.split()\n",
    "\n",
    "def avg_word_len(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    lengths = [len(word) for word in wordlist]\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "def avg_line_len(lyrics):\n",
    "    linelist = lyrics_to_linelist(lyrics)\n",
    "    lengths = [len(line) for line in linelist]\n",
    "    return sum(lengths)/len(lengths)\n",
    "\n",
    "def total_num_lines(lyrics):\n",
    "    linelist = lyrics_to_linelist(lyrics)\n",
    "    return len(linelist)\n",
    "\n",
    "def total_num_words(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    return len(wordlist)\n",
    "\n",
    "def num_contractions(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    return sum([1 if \"\\'\" in word else 0 for word in wordlist])\n",
    "\n",
    "def contraction_density(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    return sum([1 if \"\\'\" in word else 0 for word in wordlist])/len(wordlist)\n",
    "\n",
    "def vocab(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    wordlist = [stemmer.stem(word) for word in wordlist]\n",
    "    return set(wordlist)\n",
    "\n",
    "def vocab_size(lyrics):\n",
    "    return len(vocab(lyrics))\n",
    "\n",
    "def vocab_cts(lyrics):\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    wordlist = [stemmer.stem(word) for word in wordlist]\n",
    "    return dict(Counter(wordlist))\n",
    "\n",
    "lyrics = \"I love you\\nLike no other\"\n",
    "linelist = lyrics_to_linelist(lyrics)\n",
    "wordlist = linelist_to_wordlist(linelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for repetitive structure\n",
    "def lzrep_metric(uncompressed):\n",
    "    idx = 0\n",
    "    while idx < len(uncompressed):\n",
    "        uncompressed_beg = uncompressed[:idx]\n",
    "        length = 1\n",
    "        while idx + length <= len(uncompressed) and uncompressed_beg.find(uncompressed[idx:idx + length]) != -1:\n",
    "            length += 1\n",
    "        length -= 1\n",
    "\n",
    "        if length > 0:\n",
    "            idx_found = uncompressed_beg.find(uncompressed[idx:idx + length])\n",
    "            compressed.append((length, idx_found, uncompressed[idx:idx + length]))\n",
    "            idx += length\n",
    "        else:\n",
    "            compressed.append((0, 0, uncompressed[idx]))\n",
    "            idx += 1\n",
    "\n",
    "    return 1-len(compressed)/len(uncompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rhyme features\n",
    "### Adapted from https://github.com/edwadli/rapgraph/blob/master/src/rapper.py\n",
    "from nltk.corpus import cmudict\n",
    "transcr = cmudict.dict()\n",
    "_NULL_ = '_NULL_'\n",
    "phs = 'AA AE AH AO AW AY B CH D DH EH ER EY F G HH IH\\\n",
    "    IY JH K L M N NG OW OY P R S SH T TH UH UW V W Y Z'.split()\n",
    "phs_vowels = set('AA AE AH AO AW AY EH ER EY IH IY OW OY UH UW'.split())\n",
    "\n",
    "def phonemes(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    phonemes = {}\n",
    "    for word in words:\n",
    "        # get possible pronunciations from dict\n",
    "        possible_pronunciations =  transcr.get(word, [[_NULL_]])\n",
    "        if word not in transcr:\n",
    "            # TODO: generate a guess on the pronunciation\n",
    "            pass\n",
    "        # strip out emphasis on vowels\n",
    "        for pronunciation in possible_pronunciations:\n",
    "            for i in range(len(pronunciation)):\n",
    "                pronunciation[i] = ''.join(c for c in pronunciation[i] if not c.isdigit())\n",
    "        # remove repeats\n",
    "        possible_pronunciations = list(set([tuple(p) for p in possible_pronunciations]))\n",
    "        phonemes[word] = possible_pronunciations\n",
    "    return phonemes\n",
    "\n",
    "def phonemeSimilarity(ph_a, ph_b):\n",
    "    # Heuristic phoneme rhyming similarity in range [0, 1]    \n",
    "    relative_score = 0.\n",
    "    if ph_a == _NULL_ or ph_b == _NULL_:\n",
    "        return 0.\n",
    "    if ph_a == ph_b:\n",
    "        # rhyme\n",
    "        relative_score = 1.\n",
    "    elif ph_a in phs_vowels:\n",
    "        if ph_b in phs_vowels:\n",
    "            # both vowels, likely to rhyme\n",
    "            relative_score = 0.3\n",
    "    elif ph_b not in phs_vowels:\n",
    "        # both consonants, could help rhyme\n",
    "        relative_score = 0.05\n",
    "    return relative_score\n",
    "\n",
    "def alignPhonemeSequences(a_seq, b_seq):\n",
    "    # Smith-Waterman alignment with custom phoneme similarity scoring\n",
    "    GAP_PENALTY = -1.\n",
    "    MIN_SCORE = -10.\n",
    "    MAX_SCORE = 10.\n",
    "    score_range = MAX_SCORE - MIN_SCORE\n",
    "    width = len(a_seq)+1\n",
    "    height = len(b_seq)+1\n",
    "    H = [[0] * width for i in range(height)]\n",
    "    # Run the DP alg\n",
    "    for row in range(1,height):\n",
    "        for col in range(1,width):\n",
    "            relative_score = phonemeSimilarity(a_seq[col-1], b_seq[row-1])\n",
    "            align = H[row-1][col-1] + relative_score * score_range + MIN_SCORE\n",
    "            deletion = H[row-1][col] + GAP_PENALTY\n",
    "            insertion = H[row][col-1] + GAP_PENALTY\n",
    "            H[row][col] = max(0, align, deletion, insertion)\n",
    "    # extract the solution\n",
    "    # find max value in H\n",
    "    max_value = 0\n",
    "    max_row = None\n",
    "    max_col = None\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if H[row][col] >= max_value:\n",
    "                max_value = H[row][col]\n",
    "                max_row = row\n",
    "                max_col = col\n",
    "    return max_value, H\n",
    "\n",
    "def end_rhyme_score(a_seq, b_seq):\n",
    "    max_val, h = alignPhonemeSequences(a_seq, b_seq)\n",
    "    return h[-1][-1]\n",
    "\n",
    "def aligned_rhyme_score(a_seq, b_seq):\n",
    "    max_val, h = alignPhonemeSequences(a_seq, b_seq)\n",
    "    return max_val\n",
    "\n",
    "def aligned_matrix(a_seq, b_seq):\n",
    "    max_val, h = alignPhonemeSequences(a_seq, b_seq)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get line adjacency graph\n",
    "def get_rhyme_adj_graph(lyrics, thresh = 0):\n",
    "    linelist = lyrics_to_linelist(lyrics)\n",
    "    wordlist = lyrics_to_wordlist(lyrics)\n",
    "    get_phonemes = phonemes(wordlist)\n",
    "    num_wrds = len(wordlist)\n",
    "    graph = np.zeros((num_wrds, num_wrds))\n",
    "    i = 0\n",
    "    for j, line in enumerate(linelist):\n",
    "        full_phrase = line\n",
    "        if j < len(linelist)-1:\n",
    "            full_phrase = linelist[j] + linelist[j+1]\n",
    "        for k, word in enumerate(line):\n",
    "            word1 = word\n",
    "            for l, word2 in enumerate(full_phrase[k+1:]):\n",
    "                ph1 = get_phonemes[word1]\n",
    "                ph2 = get_phonemes[word2]\n",
    "                w = 0\n",
    "                for p1 in ph1:\n",
    "                    for p2 in ph2:\n",
    "                        w = max(w, aligned_rhyme_score(p1, p2))\n",
    "                graph[i, i+1+l] = w\n",
    "                graph[i+1+l, i] = w\n",
    "            i += 1\n",
    "    graph[graph <= thresh] = 0\n",
    "    return graph\n",
    "\n",
    "def edge_density(rhyme_graph, weighted=False):\n",
    "    if weighted:\n",
    "        return np.sum(rhyme_graph)/(np.size(rhyme_graph) - rhyme_graph.shape[0])\n",
    "    return np.count_nonzero(rhyme_graph)/(np.size(rhyme_graph) - rhyme_graph.shape[0])\n",
    "\n",
    "def edge_var(rhyme_graph):\n",
    "    return np.var(rhyme_graph[rhyme_graph > 0])\n",
    "\n",
    "def degree_var(rhyme_graph, weighted=False):\n",
    "    if weighted:\n",
    "        degrees = [np.sum(vertex) for vertex in rhyme_graph]\n",
    "    else:\n",
    "        degrees = [np.count_nonzero(vertex) for vertex in rhyme_graph]\n",
    "    return np.var(degrees)\n",
    "\n",
    "def degree_avg(rhyme_graph, weighted=False):\n",
    "    if weighted:\n",
    "        return np.sum(rhyme_graph)/len(rhyme_graph)\n",
    "    return np.count_nonzero(rhyme_graph)/len(rhyme_graph)\n",
    "\n",
    "def comp_size_avg(rhyme_graph):\n",
    "    return len(rhyme_graph)/nx.number_connected_components(nx.from_numpy_matrix(rhyme_graph))\n",
    "\n",
    "def num_comp(rhyme_graph):\n",
    "    return nx.number_connected_components(nx.from_numpy_matrix(rhyme_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT TRAINING, TEST, AND VALIDATION SET\n",
    "def split_train_test(df):\n",
    "    df[\"randn\"] = np.random.uniform(0, 1, df.shape[0])\n",
    "    def split_name(x):\n",
    "        if x > 0.8:\n",
    "            return \"test\"\n",
    "        if x > 0.6:\n",
    "            return \"val\"\n",
    "        else:\n",
    "            return \"train\"\n",
    "    \n",
    "    df[\"data_split\"] = np.array([split_name(n) for n in df[\"randn\"]])\n",
    "    return df\n",
    "\n",
    "SPLIT_DATA = False\n",
    "if SPLIT_DATA:\n",
    "    # Preprocess Dataframe\n",
    "    df = pd.read_pickle(ALL_DATA_FN)\n",
    "\n",
    "    if 'lyrics_stripped' not in df.columns:\n",
    "        df[\"lyrics_stripped\"] = [lyrics_strip(lyric) for lyric in df[\"lyrics\"]]\n",
    "\n",
    "    df = split_train_test(df)\n",
    "    df.to_pickle(ALL_DATA_FN)\n",
    "\n",
    "    # Get label distributions of training and test sets\n",
    "    def get_distribution(labels):\n",
    "        ct = {}\n",
    "        for lab in labels:\n",
    "            if lab in ct:\n",
    "                ct[lab] += 1\n",
    "            else:\n",
    "                ct[lab] = 1\n",
    "\n",
    "        return ct\n",
    "\n",
    "    # Calculate dataset statistics\n",
    "    print(\"Entire Dataset\")\n",
    "    print(get_distribution(df[\"genre\"]))\n",
    "    print(\"Training Set\")\n",
    "    print(get_distribution(df.query(\"data_split == 'train'\")[\"genre\"]))\n",
    "    print(\"Test Set\")\n",
    "    print(get_distribution(df.query(\"data_split == 'test'\")[\"genre\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the Lempel–Ziv compression percentage.\n",
      "Calculated the Lempel–Ziv compression percentage.\n"
     ]
    }
   ],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print('{:s} function took {:.3f} ms'.format(f.__name__, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap\n",
    "\n",
    "# Temporary Functions\n",
    "def get_rhyme_adj_graph_thresholded(x):\n",
    "    return get_rhyme_adj_graph(x, 10.0)\n",
    "\n",
    "def edge_density_weighted(x):\n",
    "    return edge_density(x, True)\n",
    "\n",
    "def degree_avg_weighted(x):\n",
    "    return degree_avg(x, True)\n",
    "\n",
    "def degree_var_weighted(x):\n",
    "    return degree_var(x, True)\n",
    "\n",
    "def edge_density_fromlyrics(x):\n",
    "    return edge_density(get_rhyme_adj_graph(x, 10.0))\n",
    "\n",
    "def edge_var_fromlyrics(x):\n",
    "    return edge_var(get_rhyme_adj_graph(x, 10.0))\n",
    "\n",
    "def degree_var_fromlyrics(x):\n",
    "    return degree_var(get_rhyme_adj_graph(x, 10.0))\n",
    "\n",
    "def degree_avg_fromlyrics(x):\n",
    "    return degree_avg(get_rhyme_adj_graph(x, 10.0))\n",
    "\n",
    "def comp_size_avg_fromlyrics(x):\n",
    "    return comp_size_avg(get_rhyme_adj_graph(x, 10.0))\n",
    "\n",
    "def edge_density_weighted_fromlyrics(x):\n",
    "    return edge_density(get_rhyme_adj_graph(x, 10.0), True)\n",
    "\n",
    "def degree_var_weighted_fromlyrics(x):\n",
    "    return degree_var(get_rhyme_adj_graph(x, 10.0), True)\n",
    "\n",
    "def degree_avg_weighted_fromlyrics(x):\n",
    "    return degree_avg(get_rhyme_adj_graph(x, 10.0), True)\n",
    "    \n",
    "# Extract independent features\n",
    "@timing\n",
    "def extract_oneoff_feats(df, recalculate=False):\n",
    "    if 'n_wrds' not in df.columns or recalculate:\n",
    "        print(\"Calculating total number of words\")\n",
    "        df[\"n_wrds\"] = df[\"lyrics_stripped\"].apply(total_num_words)\n",
    "        # Only keep songs with at least one word\n",
    "        df = df.query(\"n_wrds > 10\").copy()\n",
    "        print(\"Calculated total number of words\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if 'avg_wrd_len' not in df.columns or recalculate: \n",
    "        print(\"Calculating the average word length\")\n",
    "        df['avg_wrd_len'] = df[\"lyrics_stripped\"].apply(avg_word_len)\n",
    "        print(\"Calculated the average word length\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if 'n_lines' not in df.columns or recalculate:\n",
    "        print(\"Calculating the total number of lines\")\n",
    "        df['n_lines'] = df[\"lyrics_stripped\"].apply(total_num_lines)\n",
    "        print(\"Calculated the total number of lines\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"avg_line_len\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the average line length\")\n",
    "        df[\"avg_line_len\"] = df[\"lyrics_stripped\"].apply(avg_line_len)\n",
    "        print(\"Calculated the average line length\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"n_contractions\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the number of contractions\")\n",
    "        df[\"n_contractions\"] = df[\"lyrics_stripped\"].apply(num_contractions)\n",
    "        print(\"Calculated the number of contractions\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"contraction_density\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the density of contractions\")\n",
    "        df[\"contraction_density\"] = df[\"lyrics_stripped\"].apply(contraction_density)\n",
    "        print(\"Calculated the density of contractions\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"vocab_size\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the size of the vocabulary\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"vocab_size\"] = pool.map(vocab_size, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the size of the vocabulary\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"vocab_cts\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the counts of the vocabulary\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"vocab_cts\"] = pool.map(vocab_cts, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the counts of the vocabulary\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "#     if \"rhyme_graph\" not in df.columns or recalculate:\n",
    "#         df[\"rhyme_graph\"] = df[\"lyrics_stripped\"].apply(get_rhyme_adj_graph_thresholded)\n",
    "#         print(\"Calculated the rhyme graph\")\n",
    "#         df.to_pickle(ALL_DATA_FN)\n",
    "#         return df\n",
    "\n",
    "    if \"edge_density\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the edge density\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"edge_density\"] = pool.map(edge_density_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the edge density\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"edge_density_weighted\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the weighted edge density\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"edge_density_weighted\"] = pool.map(edge_density_weighted_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the weighted edge density\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"edge_weight_var\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the edge weight variance\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"edge_weight_var\"] = pool.map(edge_var_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the edge weight variance\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"degree_var\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the variance of the vertex degrees\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"degree_var\"] = pool.map(degree_var_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the variance of the vertex degrees\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"degree_var_weighted\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the variance of the weighted vertex degrees\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"degree_var_weighted\"] = pool.map(degree_var_weighted_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the variance of the weighted vertex degrees\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"degree_avg\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the average vertex degree\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"degree_avg\"] = pool.map(degree_avg_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the average vertex degree\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"degree_avg_weighted\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the average weighted vertex degree\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"degree_avg_weighted\"] = pool.map(degree_avg_weighted_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the average weighted vertex degree\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    if \"comp_size_avg\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the average size of a connected component\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"comp_size_avg\"] = pool.map(comp_size_avg_fromlyrics, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the average size of a connected component\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "    \n",
    "    if \"lz_comp_ratio\" not in df.columns or recalculate:\n",
    "        print(\"Calculating the Lempel–Ziv compression percentage.\")\n",
    "        with Pool(processes=30) as pool:\n",
    "            df[\"lz_comp_ratio\"] = pool.map(lzrep_metric, df[\"lyrics_stripped\"])\n",
    "        print(\"Calculated the Lempel–Ziv compression percentage.\")\n",
    "        df.to_pickle(ALL_DATA_FN)\n",
    "        return df\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.read_pickle(ALL_DATA_FN)\n",
    "while True:\n",
    "    df = extract_oneoff_feats(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7905,)\n"
     ]
    }
   ],
   "source": [
    "# Calculate tf-idf data\n",
    "def get_topk(df, k=1000):\n",
    "    df_train = df.query(\"data_split == 'train'\").copy()\n",
    "    df_test = df.query(\"data_split == 'test'\").copy()\n",
    "    df_val = df.query(\"data_split == 'val'\").copy()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "    corpus = []\n",
    "    for genre in GENRES:\n",
    "        df_genre = df_train.query(\"genre == '%s'\" % genre).copy()\n",
    "        tokenized = df_genre[\"lyrics_stripped\"].apply(lambda x: \" \".join(list(set(tokenizer(x)))))\n",
    "        corpus.append(\" \".join(tokenized))\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    top_k_wrds = []\n",
    "    for i, genre in enumerate(GENRES):\n",
    "        words_freq = [(word, X[i, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        words = [pair[0] for pair in words_freq]\n",
    "        top_k_wrds += words[:k]\n",
    "    top_k_wrds = list(set(top_k_wrds))\n",
    "    return top_k_wrds\n",
    "\n",
    "def get_tfidf(df, vocab):\n",
    "    df_train = df.query(\"data_split == 'train'\").copy()\n",
    "    df_test = df.query(\"data_split == 'test'\").copy()\n",
    "    df_val = df.query(\"data_split == 'val'\").copy()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "    corpus_train = df_train[\"lyrics_stripped\"]\n",
    "    X_train = vectorizer.fit_transform(corpus_train)\n",
    "    \n",
    "    X_all = vectorizer.transform(df[\"lyrics_stripped\"])\n",
    "    return X_all, vectorizer\n",
    "\n",
    "def calculate_tfidf(df, k=4000):\n",
    "    vocab = get_topk(df, k)\n",
    "    x, vectorizer = get_tfidf(df, vocab)\n",
    "    x_dense = np.array(x.todense())\n",
    "    df[\"topk\"] = [x_dense[i,] for i in range(x_dense.shape[0])]\n",
    "    df.to_pickle(ALL_DATA_FN)\n",
    "    return vectorizer\n",
    "    \n",
    "CALCULATE_TFIDF = True\n",
    "if CALCULATE_TFIDF:\n",
    "    k = 4000\n",
    "    df = pd.read_pickle(ALL_DATA_FN)\n",
    "    vectorizer = calculate_tfidf(df, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizer vocabulary\n",
    "vocab = vectorizer.vocabulary_\n",
    "vocab = {vocab[v]:v for v in vocab}\n",
    "\n",
    "vocab = [vocab[i] for i in range(len(vocab))]\n",
    "with open('topk_vocab', 'wb') as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7904"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(ALL_DATA_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_count(df):\n",
    "    df_train = df.query(\"data_split == 'train'\").copy()\n",
    "    df_test = df.query(\"data_split == 'test'\").copy()\n",
    "    df_val = df.query(\"data_split == 'val'\").copy()\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    corpus_train = df_train[\"lyrics_stripped\"]\n",
    "    X_train = vectorizer.fit_transform(corpus_train)\n",
    "    \n",
    "    X_all = vectorizer.transform(df[\"lyrics_stripped\"])\n",
    "    return X_all\n",
    "\n",
    "def calculate_bag_of_words(df):\n",
    "    x = get_raw_count(df)\n",
    "    x_raw_ct = np.array(x.todense())\n",
    "    x_one_hot = (x_raw_ct > 0).astype(\"int64\")\n",
    "    \n",
    "    df[\"raw_count\"] = [x_raw_ct[i,] for i in range(x_raw_ct.shape[0])]\n",
    "    df[\"one_hot\"] = [x_one_hot[i,] for i in range(x_one_hot.shape[0])]\n",
    "    df.to_pickle(ALL_DATA_FN)\n",
    "\n",
    "CALCULATE_BAGOFWORDS = False\n",
    "if CALCULATE_BAGOFWORDS:\n",
    "    df = pd.read_pickle(ALL_DATA_FN)\n",
    "    calculate_bag_of_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_DIM = 100\n",
    "\n",
    "def get_word2vec(df):\n",
    "    df_train = df.query(\"data_split == 'train'\").copy()\n",
    "    df_test = df.query(\"data_split == 'test'\").copy()\n",
    "    df_val = df.query(\"data_split == 'val'\").copy()\n",
    "    \n",
    "    lyrics = list(df_train[\"lyrics_stripped\"])\n",
    "    corpus = [lyrics_to_linelist(lyrics[i]) for i in range(len(lyrics))]\n",
    "    corpus = list(itertools.chain.from_iterable(corpus))\n",
    "    model = Word2Vec(corpus)\n",
    "    model.save(\"word2vec.model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_word2vec_avg(words, model):\n",
    "    words_list = lyrics_to_wordlist(words)\n",
    "    vec_list = [model.wv[word] if word in model.wv.vocab else None for word in words_list]\n",
    "    vec_list = [x for x in vec_list if x is not None]\n",
    "    if len(vec_list) == 0:\n",
    "        avg = [0]*VEC_DIM\n",
    "    else:\n",
    "        avg = sum(vec_list) / len(vec_list)\n",
    "    return avg\n",
    "\n",
    "def calculate_word2vec_avg(df):\n",
    "    if isfile(\"word2vec.model\"):\n",
    "        model = Word2Vec.load(\"word2vec.model\")\n",
    "    else:\n",
    "        model = get_word2vec(df)\n",
    "    df[\"word2vec_avg\"] = [get_word2vec_avg(words, model) for words in df[\"lyrics_stripped\"]]\n",
    "    df.to_pickle(ALL_DATA_FN)\n",
    "\n",
    "CALCULATE_WORD2VEC_AVG = True\n",
    "if CALCULATE_WORD2VEC_AVG:\n",
    "    df = pd.read_pickle(ALL_DATA_FN)\n",
    "    calculate_word2vec_avg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(ALL_DATA_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6138211382113821"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
